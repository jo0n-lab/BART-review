# BART : Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

## Keywords

`GPT`; `BERT`; `AR(Auto Regression)`; `AE(Auto Encoding)`; `Bidirectional`; `transformer`;

## Introduction

BERTì˜ ì„±ëŠ¥ì„ generalize í•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ë°©ì‹ì„ ì ìš©í•œë‹¤.

- BART : pretrain model = Bidirectional(generalize BERT) + Auto-regressive(GPT)
    - ë¬´ì‘ìœ„ì ìœ¼ë¡œ corrupted sentence(`AE`) : ìµœê³ ì˜ ì„±ëŠ¥!!
        - ì¡°ì‘ë³€ì¸(hyper-parameters) : `span(spanBERT)` `order(Xlnet)`
        - `MLM` `NSP` objective ì˜ í™˜ê²½ generalize íš¨ê³¼
    - sequence-to-sequence model to reconstruct sentence(`AR`)
        
        ![Untitled](./source/Untitled.png)
        
    
    <aside>
    ğŸ‘‰ encoder(AE), decoder(AR) ì˜ objective ë¥¼ ì„œë¡œ ë‹¤ë¥´ê²Œ ì„¤ê³„
    
    </aside>
    
- ê°•ì (when fine-tuned byâ€¦) : ì„ ì‹¤í—˜ì—ì„œ í™•ì¸
    - text generation
    - comprehension
- New way of Fine-tuning
    - machine translation
        - ê¸°ì¡´ì— ì—†ì—ˆë˜ transformer layer ìœ„ì— BART modelì„ ì–¹ëŠ”ë‹¤.(encoder ìœ„ì—)
            - foreign language â†’ noised english (can say target-side LM objective)
            - strong performance in back translation
                - what is back translation
                    
                    ![Untitled](./source/Untitled%201.png)
                    
                    ![Untitled](./source/Untitled%202.png)
                    
                

## Model

- Architecture
    - sequence-to-sequence Transformer from `GPT` but 
    (`Activation Function` & `Initialization`)
        - ReLU â†’ GeLU
        - initialize parameters from $\mathcal{N}(0,0.02)$
            - 0 ì— ê·¼ì ‘í•˜ê³  robust í•˜ê²Œ ì´ˆê¸°í™”í–ˆë‹¤ëŠ” ê±°
    - Base : encoder : 6, decoder : 6 (layers)   Large : encoder :12, decoder :12 (layers)
    `BERT`ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì´ 
    (`Encoder`, `Decoder` architecture by N of `params`)
        - decoder ê°ê°ì˜ ë ˆì´ì–´ì—ì„œ encoder ì˜ ëë‹¨ hidden layer(outputì „ì—) cross-attention
        - BERT ë°©ì‹ì˜ word-prediction ì „ì— feed-forward network ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            - what is feed-forward network
                
                ![Untitled](./source/Untitled%203.png)
                
                Feed Forward neural network helps a lot in finding the more contextual information related to particular pairs of words in sequences . It is not very clear why we need another feed forward network if we already have an attention layer which does the same job but it helps to improve the accuracy of the model you can seeÂ [here](https://youtu.be/YIEe7d7YqaU)
                Â if you wanna know more detail about it.
                
        - ì´ë ‡ê²Œ í–ˆì„ ë•Œ 10% parameter ë§ì•„ì§

- Pre-training BART : trained by corrupting docs & optimizing reconstruction loss
    - decoder output : $\hat{y}$. original doc : $y$
    - applicable to all kinds of corrupted sentences(currently presented)
        - all sentences masked â†’ LM
    - Token Masking `MLM(BERT)`
    - Token Deletion : some tokens deleted from input
        - predict where the deleted tokens
    - Test Infilling `span(spanBERT)`
        - length : poisson distribution $(\lambda =3)$ : how many are gone
            - fixed span length
            - span â†’ masked token(sigle token)
            - learn how many are gone(not masked tokens, just tokens)
            
            - spanBERT
                - span from different distribution
                - span â†’ masked tokenâ€™sâ€™(several tokens)
            
            ```python
            while len(mask) < mask_num:
            	span_len = np.random.choice(self.lens, p=self.len_distrib)
            
            									....
            ```
            
    - Sentence Permutation `order(Xlnet)`
        - full stop(ë§ˆì¹¨í‘œ) sentences shuffled in random order
    - Document Rotation
        - selected pivot : front, remainder : rotated
        
        ![Untitled](source/Untitled%204.png)
        

## Fine-tuning

- Sequence Classification Tasks :Sequence classification isÂ **the task of predicting a class label given a sequence of observations**
    - input : identical â†’ encoder & decoder
    - output : decoder ëë‹¨ hidden state ì˜ í† í°($\hat{y}(class)$) â†’ multi-class linear classifier
    
    ![Untitled](source/Untitled%205.png)
    

- Token Classification Tasks
    - input : complete document â†’ encoder & decoder
    - output : top hidden state representation of words from decoder $\hat{y}^{(1,T)}$
    
- Sequence Generation Tasks
    - since `AR` (`GPT`)
        - abstractive QA
        - summerization
    - input sequences : information copied and manipulated(related to denoising pretrain objective)
    - input : input sequences â†’ encoder
    - output â†decoder(`AR`)
    
- Machine Translation
    - using entite `BART` model is superier than presented models(using lone encoder)
        - single pretrained decoder
        - pretrained encoder + new encoding layer(train : Bitext) â—ï¸this is not pretrained
            - pretrained encoder embedding layer replaced by a new layer(randomly initialized)
            - trained end-to-end (task based learningâ€”no transfer)
                
                ![Untitled](source/Untitled%206.png)
                
                - foreign â†’ English
                - freeze almost params except the layer
                    - positional embeddings
                    - self-attention input projection matrix of first of whole encoder layer
                    (this is why â€œBART model is stacked above a few ad- ditional transformer layers.â€)
                - train all params (for small iter_nums)
                

## Comparing Pre-training Objectives

Letâ€™s compare our(BARTâ€™s) denoising performance with others

Base : encoder(6 layers) decoder(6 layers) hidden size(768)

- Comparison Objective
    - deciding obective to compare was challenging due to different train set, architectural difference, fine-tuning procedures (all because of different environment)
        - applicate to recently strong pre-training approaches for discriminative & generation tasks
        - aim to control unrelated pre-train objectives
        - however we â€˜did minor changes separatly to objectivesâ€™ to improve performance
        (viewpoint from strict `pre-train`)
            - $\texttt{learing\_rate}$
            - usage of layer normalization
            
- Language Model
    - `GPT` $\sim$ `BART decoder` - (cross-attention)

- Permutated Language Model(based on `XLNet` )
    - 1/6 of tokens and shuffle
    - (but not-use positional embedding, across segments)

- Masked Language Model (based on `BERT`)
    - 15% of token â†’ masked token

- Multitask Masked Language Model(based on `UniLM`)
    - additional self-attention masks
        - left-to-right : 1/6
        - right-to-left : 1/6
        - unmasked : 1/3
        - ??

- Masked Seq-to-Seq (based on `MASS` )
    - mask span(50% tokens)
    - train sequence to sequence (predict : masked tokens)# BART-review
